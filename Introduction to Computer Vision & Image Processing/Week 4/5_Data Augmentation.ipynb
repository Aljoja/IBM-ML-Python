{"cells":[{"cell_type":"markdown","id":"6fda0c4a-251d-4430-9906-17281945e8f4","metadata":{},"source":["<a href=\"http://cocl.us/pytorch_link_top\">\n","    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \">\n","</a> \n"]},{"cell_type":"markdown","id":"0a3337f3-0c86-4107-aeb4-2d3b94b969cb","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"58ebaf14-f65d-4102-9fba-7ddbe095e39e","metadata":{},"source":["<h1>Data Augmentation </h1> \n"]},{"cell_type":"markdown","id":"706f9c4d-12f9-42e4-9e18-d4ee745df459","metadata":{},"source":["<h2>Table of Contents</h2>\n","\n","In this lab, we train a Convolutional Neural Network with Regular data and Augmented data. The purpose of this lab is to show that the Augmented data improves generalization performance.\n","\n","Think of a scenario where a drone has to take a picture of an object. The drone is moving and the object can also possibly be moving. When an image is taken we arent always going to get perfect images. The subject may not be perfectly centered in the image or the subject may be rotated in the image. In this case, a model trained on perfectly centered or rotated images won't perform well. This is why we train a model on rotated data so it can perform well on imperfect images.\n","\n","In this assignment, we will use a dataset of digit images. We will have two models one trained on non rotated digits and one trained on rotated images and then we will test the models on a rotated testing dataset which will be more realistic and robust in terms of our scenario above.\n","\n","<ul>\n","<li><a href=\"#Makeup_Data\">Get Some Data</a></li>\n","<li><a href=\"#CNN\">Convolutional Neural Network</a></li>\n","<li><a href=\"#R_training_data\">Rotated Training Data</a></li>\n","\n","</ul>\n","<p>Estimated Time Needed: <strong>25 min</strong> 14 min to train model </p>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"e1dbb076-e306-4967-b7bd-5a95c0ca1e97","metadata":{},"source":["<h2>Preparation</h2>\n"]},{"cell_type":"code","execution_count":1,"id":"ddb8c544-0b26-4ca6-ad75-ab6a40fbddd3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in c:\\users\\amurd\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (1.10.2)\n","Requirement already satisfied: torchvision in c:\\users\\amurd\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (0.11.3)\n","Requirement already satisfied: torchaudio in c:\\users\\amurd\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (0.10.2)\n","Requirement already satisfied: typing-extensions in c:\\users\\amurd\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from torch) (4.1.1)\n","Requirement already satisfied: dataclasses in c:\\users\\amurd\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from torch) (0.8)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\amurd\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from torchvision) (8.3.1)\n","Requirement already satisfied: numpy in c:\\users\\amurd\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from torchvision) (1.19.2)\n"]}],"source":["!pip3 install torch torchvision torchaudio"]},{"cell_type":"code","execution_count":2,"id":"402b3ed0-ac4f-4821-b696-88251d981760","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["'wget' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["# Download a Pretrained Model because training takes a long time\n","!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/meet_up/12.02.2020/normal.pt"]},{"cell_type":"code","execution_count":3,"id":"cdb3b8eb-ee20-45c6-bd2f-f021e8cbf1c7","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["'wget' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["# Download a Pretrained Model Trained on Augmented Data because training takes a long time\n","!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/meet_up/12.02.2020/rotated_data.pt"]},{"cell_type":"code","execution_count":4,"id":"a20cb751-fb6d-4108-8b70-4979f3e9acc1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting Pillow==6.2.2\n","  Downloading Pillow-6.2.2-cp36-cp36m-win_amd64.whl (2.0 MB)\n","Installing collected packages: Pillow\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 8.3.1\n","    Uninstalling Pillow-8.3.1:\n","      Successfully uninstalled Pillow-8.3.1\n","Successfully installed Pillow-6.2.2\n"]}],"source":["# Library to Show Images\n","!pip install Pillow==6.2.2"]},{"cell_type":"markdown","id":"2a18628b-f05c-46c5-9643-8ab1da4adda6","metadata":{},"source":["***Restart the kernel***\n"]},{"cell_type":"code","execution_count":5,"id":"615a98a9-8504-474a-8fe9-6df9aec92691","metadata":{},"outputs":[],"source":["# Import the libraries we need to use in this lab\n","# Using the following line code to install the torchvision library\n","# !conda install -y torchvision\n","\n","# PyTorch Library\n","import torch \n","# PyTorch Neural Network Library\n","import torch.nn as nn\n","# Allows us to transform data\n","import torchvision.transforms as transforms\n","# Used to graph data and loss curves\n","import matplotlib.pylab as plt\n","# Allows us to use arrays to manipulate and store data\n","import numpy as np\n","# Allows us to download the dataset\n","import torchvision.datasets as dsets\n","# Allows us to access the filesystem\n","import os  "]},{"cell_type":"markdown","id":"c5484ece-58ac-4596-82c5-a20ee9fb9192","metadata":{},"source":["Some useful functions for plotting \n"]},{"cell_type":"markdown","id":"7bb6e2c5-f49c-415a-a5e7-6c8ffb0f65ee","metadata":{},"source":["Plot Cost and Accuracy vs Epoch Graph\n"]},{"cell_type":"code","execution_count":6,"id":"d399bcc1-69c5-43fc-a16c-37ac89ce95cf","metadata":{},"outputs":[],"source":["def plot_cost_accuracy(checkpoint):\n","\n","# Plot the cost and accuracy\n","\n","    fig, ax1 = plt.subplots()\n","    color = 'tab:red'\n","    ax1.plot(checkpoint['cost'], color=color)\n","    ax1.set_xlabel('epoch', color=color)\n","    ax1.set_ylabel('Cost', color=color)\n","    ax1.tick_params(axis='y', color=color)\n","    \n","    ax2 = ax1.twinx()  \n","    color = 'tab:blue'\n","    ax2.set_ylabel('accuracy', color=color) \n","    ax2.set_xlabel('epoch', color=color)\n","    ax2.plot( checkpoint['accuracy'], color=color)\n","    ax2.tick_params(axis='y', color=color)\n","    fig.tight_layout()"]},{"cell_type":"markdown","id":"e9e7ff77-981e-4ab3-93a2-ed6525211767","metadata":{},"source":["Define the function <code>show_data</code> to plot out data samples as images.\n"]},{"cell_type":"code","execution_count":7,"id":"4ee6c5b9-e14c-490e-a6ac-213bb19fac97","metadata":{},"outputs":[],"source":["def show_data(data_sample):\n","    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n","    plt.title('y = '+ str(data_sample[1]))"]},{"cell_type":"markdown","id":"8d273df3-33bc-48c8-ad83-d17acffadf9b","metadata":{},"source":["Plot first 5 misclassified samples \n"]},{"cell_type":"code","execution_count":8,"id":"018be407-fd5f-4d1a-b4a3-d329149788f0","metadata":{},"outputs":[],"source":["def plot_mis_classified(model, dataset):\n","    count=0\n","    for x, y in torch.utils.data.DataLoader(dataset=dataset, batch_size=1):\n","        z = model(x)\n","        _, yhat = torch.max(z, 1)\n","        if yhat != y:\n","            show_data((x, y))\n","            plt.show()\n","            count += 1\n","        if count >= 5:\n","            break "]},{"cell_type":"markdown","id":"eb80eb00-a2ea-407f-9327-fa7e0cc35d3f","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"dea09604-f4c2-421d-8c27-e3ba71070ce4","metadata":{},"source":["<h2 id=\"Makeup_Data\">Load Data</h2> \n"]},{"cell_type":"markdown","id":"8908b2d7-690a-480e-a0cb-4a3f6584172b","metadata":{},"source":["We create a transform object  <code>compose</code> one will resize the image and convert it to a tensor, the second will also rotate the image Randomly rotate the image.\n"]},{"cell_type":"code","execution_count":9,"id":"f60f1a7b-5df7-49fa-b007-834d2c95eb95","metadata":{},"outputs":[],"source":["# Size of the images are 16 by 16\n","IMAGE_SIZE = 16\n","\n","# Creating a group of transformations to created a rotated dataset\n","# Resizes the images, randomly rotates it, and then converts it to a tensor\n","compose_rotate = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),transforms.RandomAffine(45), transforms.ToTensor()])\n","\n","# Creating a group of transformations to created a non rotated dataset\n","# Resizes the images then converts it to a tensor\n","compose = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"]},{"cell_type":"markdown","id":"3653e4d0-72e3-4051-ad0d-cf11dbb4e07e","metadata":{},"source":["Load the training dataset by setting the parameters <code>train</code> to <code>True</code>. We use the transform defined above, one with rotated data one without.\n"]},{"cell_type":"code","execution_count":10,"id":"518fb897-9a71-40f7-9fe4-8d6044126bbe","metadata":{},"outputs":[],"source":["# The transform parameters is set to the corresponding compose\n","train_dataset_rotate = dsets.MNIST(root='./data', train=True, download=True, transform=compose_rotate)\n","train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=compose)"]},{"cell_type":"markdown","id":"8cdf36c2-ec86-4ba8-9019-ad76d46632bd","metadata":{},"source":["Load the testing dataset by setting the parameters train to <code>False</code>, where the data is <b>ALL</b> rotated.\n"]},{"cell_type":"code","execution_count":11,"id":"b2d0745e-04a5-42ae-8a09-50f317ee143e","metadata":{},"outputs":[],"source":["# Load the testing dataset\n","validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=compose_rotate)"]},{"cell_type":"markdown","id":"412915ad-1f31-4a14-9576-3c8b99082043","metadata":{},"source":["Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image.\n"]},{"cell_type":"markdown","id":"dcf7cafd-70fa-4ded-ad18-8f0df17465d4","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.2.1imagenet.png\" width=\"550\" alt=\"MNIST data image\">\n"]},{"cell_type":"markdown","id":"7487de01-2d6f-4265-aa26-8facc69ec913","metadata":{},"source":["Plot the first sample \n"]},{"cell_type":"code","execution_count":12,"id":"ddd738ee-592c-4c4a-b177-aa435401c51f","metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# The image for the first data sample\n","show_data(train_dataset[0])"]},{"cell_type":"markdown","id":"43cbb3a6-6abb-46b7-936b-034224d7b0bf","metadata":{},"source":["Print out the first label, as we can see this digit is a five\n"]},{"cell_type":"code","execution_count":13,"id":"63f2567f-a900-4d4d-89fd-ee61c527e1e3","metadata":{},"outputs":[{"data":{"text/plain":["5"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# The label for the first data element\n","train_dataset[0][1]"]},{"cell_type":"markdown","id":"90d26b42-c315-437f-9e02-7addd4abc0ec","metadata":{},"source":["Now let's plot the first sample of the rotated training dataset\n"]},{"cell_type":"code","execution_count":14,"id":"06d1a9db-8b32-4591-970c-59c0b607e200","metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["show_data(train_dataset_rotate[0])"]},{"cell_type":"markdown","id":"05a6b202-c01e-4033-aea3-aa97894b13f6","metadata":{},"source":["We can see that this is the same sample but it is rotated as we wanted\n"]},{"cell_type":"markdown","id":"6daf57bb-1ca1-4904-8a9e-0652f8eff41f","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"af433d55-0677-49aa-bc3b-cc70cd26a19c","metadata":{},"source":["<h2 id=\"CNN\">Build a Convolutional Neural Network Class</h2>\n"]},{"cell_type":"markdown","id":"fddc070e-d187-4123-9920-1f0291ec5435","metadata":{},"source":["Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layers.\n"]},{"cell_type":"code","execution_count":15,"id":"c803c59a-ff4d-4540-a756-6dbb32563a20","metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n","    \n","    # Contructor\n","    def __init__(self, out_1=16, out_2=32):\n","        super(CNN, self).__init__()\n","        \n","        # The reason we start with 1 channel is because we have a single black and white image\n","        # Channel Width after this layer is 16\n","        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n","        # Channel Wifth after this layer is 8\n","        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n","        \n","        # Channel Width after this layer is 8\n","        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n","        # Channel Width after this layer is 4\n","        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n","        # In total we have out_2 (32) channels which are each 4 * 4 in size based on the width calculation above. Channels are squares.\n","        # The output is a value for each class\n","        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n","    \n","    # Prediction\n","    def forward(self, x):\n","        # Puts the X value through each cnn, relu, and pooling layer and it is flattened for input into the fully connected layer\n","        x = self.cnn1(x)\n","        x = torch.relu(x)\n","        x = self.maxpool1(x)\n","        x = self.cnn2(x)\n","        x = torch.relu(x)\n","        x = self.maxpool2(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        return x\n","\n","    # Outputs result of each stage of the CNN, relu, and pooling layers\n","    def activations(self, x):\n","        # Outputs activation this is not necessary\n","        z1 = self.cnn1(x)\n","        a1 = torch.relu(z1)\n","        out = self.maxpool1(a1)\n","        \n","        z2 = self.cnn2(out)\n","        a2 = torch.relu(z2)\n","        out1 = self.maxpool2(a2)\n","        out = out.view(out.size(0),-1)\n","        return z1, a1, z2, a2, out1,out"]},{"cell_type":"markdown","id":"e9b8b0bf-7e18-4bfd-a321-b3d78a7132bf","metadata":{},"source":["<h2 id=\"regular_data\">Regular Data</h2> \n"]},{"cell_type":"markdown","id":"bc7039cb-0d35-40c2-a08d-95d77bff3258","metadata":{},"source":["Define the Convolutional Neural Network Classifier, Criterion function, Optimizer, and Train the Model\n"]},{"cell_type":"code","execution_count":16,"id":"9ad05fe4-ca68-48bb-8045-ca2e8d878189","metadata":{},"outputs":[],"source":["# Create the model object to be trained on regular data using CNN class\n","model = CNN(out_1=16, out_2=32)"]},{"cell_type":"markdown","id":"c0a12002-aafe-4e42-9c0e-0d783ffe5f2b","metadata":{},"source":["Define the loss function, the optimizer, and the dataset loader \n"]},{"cell_type":"code","execution_count":17,"id":"64183a14-9952-4998-94dc-63de21ea0bc6","metadata":{},"outputs":[],"source":["# We create a criterion which will measure loss\n","criterion = nn.CrossEntropyLoss()\n","learning_rate = 0.1\n","# Create an optimizer that updates model parameters using the learning rate and gradient\n","optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n","# Create a Data Loader for the training data with a batch size of 100 \n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n","# Create a Data Loader for the rotated validation data with a batch size of 5000 \n","validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"]},{"cell_type":"markdown","id":"31cf8724-4607-4d4b-91bc-319104381bb8","metadata":{},"source":["This cell will train the model, we will comment it out as it takes a long time to run. You can change the block type from Raw to Code and run it or you can load the trained model in the next cell. Notice that we are not only training and saving the model here but we are also keeping track of important data like the cost and accuracy throughout the training process.\n"]},{"cell_type":"raw","id":"3c56a6d7-bf3b-4e29-a7a9-abc52f52be75","metadata":{},"source":["# Train the model\n","import os  \n","\n","# Location to save data\n","file_normal = os.path.join(os.getcwd(), 'normal.pt')\n","\n","# All the data we are saving\n","checkpoint={\n","            # Saving the number of epochs the models was trained for\n","            'epoch': None,\n","            # Saving the models parameters which will allow us to recreate the trained model\n","            'model_state_dict': None,\n","            # Saving the optimizers parameters\n","            'optimizer_state_dict': None,\n","            # Saving the loss on the training dataset for the last batch of the last epoch\n","            'loss': None,\n","            # Saving the cost on the training dataset for each epoch\n","            'cost': [],\n","            # Saving the accuracy for the testing dataset for each epoch\n","            'accuracy': []}\n","            \n","# Number of epochs to train model\n","n_epochs = 5\n","\n","# Size of the testing dataset\n","N_test = len(validation_dataset)\n","\n","# Training for the number of epochs we want\n","for epoch in range(n_epochs):\n","    # Variable to keep track of cost for each epoch\n","    cost = 0\n","    # For each batch in the training dataset\n","    for x, y in train_loader:\n","        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n","        optimizer.zero_grad()\n","        # Makes a prediction on the image\n","        z = model(x)\n","        # Calculate the loss between the prediction and actual class\n","        loss = criterion(z, y)\n","        # Calculates the gradient value with respect to each weight and bias\n","        loss.backward()\n","        # Updates the weight and bias according to calculated gradient value\n","        optimizer.step()\n","      \n","        # Saves the number of epochs we trained for  \n","        checkpoint['epochs'] = n_epochs\n","        # Saves the models parameters\n","        checkpoint['model_state_dict'] = model.state_dict()\n","        # Saves the optimizers paramters\n","        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n","        # Saves the loss for the last batch so ultimately this will be the loss for the last batch of the last epoch\n","        checkpoint['loss'] = loss\n","        # Accumulates the loss\n","        cost += loss.item()\n","        \n","     \n","    # Counter for the correct number of predictions        \n","    correct = 0\n","        \n","    # For each batch in the validation dataset\n","    for x_test, y_test in validation_loader:\n","        # Make a prediction\n","        z = model(x_test)\n","        # Get the class that has the maximum value\n","        _, yhat = torch.max(z.data, 1)\n","        # Counts the number of correct predictions made\n","        correct += (yhat == y_test).sum().item()\n"," \n","    accuracy = correct / N_test\n","    print(accuracy)\n","    # Appends the cost of the epoch to a list\n","    checkpoint['cost'].append(cost) \n","    # Appends the accuracy of the epoch to a list\n","    checkpoint['accuracy'].append(accuracy)\n","    # Saves the data in checkpoint to the file location\n","    torch.save(checkpoint, file_normal) "]},{"cell_type":"markdown","id":"8672d619-176f-4c22-b172-94949bd2667e","metadata":{},"source":["<h3 id=\"Result\">Analyze Results</h3> \n"]},{"cell_type":"markdown","id":"1aa5eeef-1dc3-49e2-91f7-cdd87e6a1727","metadata":{},"source":["Loads the data which is saved in `normal.pt`\n"]},{"cell_type":"code","execution_count":18,"id":"da3755f1-5ba4-44ee-877d-d826a2e7f612","metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'c:\\\\Users\\\\amurd\\\\Desktop\\\\GitRepositories\\\\IBM-ML-Python\\\\Introduction to Computer Vision & Image Processing\\\\Week 4\\\\normal.pt'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-18-77e3c4ecde14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcheckpoint_normal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'normal.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32mc:\\Users\\amurd\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\amurd\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\amurd\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\amurd\\\\Desktop\\\\GitRepositories\\\\IBM-ML-Python\\\\Introduction to Computer Vision & Image Processing\\\\Week 4\\\\normal.pt'"]}],"source":["checkpoint_normal = torch.load(os.path.join(os.getcwd(),'normal.pt'))"]},{"cell_type":"markdown","id":"e7528c23-db6e-40ad-9ffd-09ee5fbef58e","metadata":{},"source":["Plot Accuracy and Cost vs Epoch Graph\n"]},{"cell_type":"code","execution_count":null,"id":"9690fa4d-e11e-4abe-8aeb-a9705ec6b826","metadata":{},"outputs":[],"source":["# Using the helper function defined at the top and the cost and accuracy lists that we saved\n","plot_cost_accuracy(checkpoint_normal)"]},{"cell_type":"markdown","id":"51fe45f4-2d22-4224-b3a4-be6f47abb1ed","metadata":{},"source":["Five misclassified samples\n"]},{"cell_type":"code","execution_count":null,"id":"69678396-1012-4a90-99d1-adc090c5bcd6","metadata":{},"outputs":[],"source":["# Using the model parameters we saved we load them into a model to recreate the trained model\n","model.load_state_dict(checkpoint_normal['model_state_dict'])\n","# Setting the model to evaluation mode\n","model.eval()\n","# Using the helper function plot the first five misclassified samples\n","plot_mis_classified(model,validation_dataset)"]},{"cell_type":"markdown","id":"42e6f9cf-2a62-45ab-978f-4fc103d75bcc","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"764d2743-ad16-415b-b8fb-50fb19473a0d","metadata":{},"source":["<h2 id=\"R_training_data\">Rotated Training Data</h2> \n"]},{"cell_type":"markdown","id":"cad7b4f2-17c7-4625-b779-71a27adbdef1","metadata":{},"source":["Plot the loss and accuracy on the validation data:\n"]},{"cell_type":"code","execution_count":null,"id":"18343650-4077-468a-a520-4e58fa4ab3c6","metadata":{},"outputs":[],"source":["# Create the model object using CNN class\n","model_r = CNN(out_1=16, out_2=32)\n","# We create a criterion which will measure loss\n","criterion = nn.CrossEntropyLoss()\n","learning_rate = 0.1\n","# Create an optimizer that updates model parameters using the learning rate and gradient\n","optimizer = torch.optim.SGD(model_r.parameters(), lr = learning_rate)\n","# Create a Data Loader for the rotated training data with a batch size of 100 \n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset_rotate, batch_size=100)\n","# Create a Data Loader for the rotated validation data with a batch size of 5000 \n","validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"]},{"cell_type":"markdown","id":"7444c668-c4c1-4fac-be8d-a8e275c6463a","metadata":{},"source":["This cell will train the model, we will comment it out as it takes a long time to run. You can change the block type from Raw to Code and run it or you can load the trained model in the next cell.\n"]},{"cell_type":"raw","id":"ec5d4294-a16d-4e2f-8684-fe844f9fdf9f","metadata":{},"source":["# Location to save data\n","file_rotated = os.path.join(os.getcwd(), 'rotated_data.pt')\n","\n","# All the data we are saving\n","checkpoint={\n","            # Saving the number of epochs the models was trained for\n","            'epoch': None,\n","            # Saving the models parameters which will allow us to recreate the trained model\n","            'model_state_dict': None,\n","            # Saving the optimizers parameters\n","            'optimizer_state_dict': None,\n","            # Saving the loss on the training dataset for the last batch of the last epoch\n","            'loss': None,\n","            # Saving the cost on the training dataset for each epoch\n","            'cost': [],\n","            # Saving the accuracy for the testing dataset for each epoch\n","            'accuracy': []}\n","            \n","# Number of epochs to train model            \n","n_epochs = 5\n","\n","# Size of the testing dataset\n","N_test = len(validation_dataset)\n","\n","# Training for the number of epochs we want\n","for epoch in range(n_epochs):\n","    # Variable to keep track of cost for each epoch\n","    cost = 0\n","    # For each batch in the training dataset\n","    for x, y in train_loader:\n","        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n","        optimizer.zero_grad()\n","        # Makes a prediction on the image\n","        z = model_r(x)\n","        # Calculate the loss between the prediction and actual class\n","        loss = criterion(z, y)\n","        # Calculates the gradient value with respect to each weight and bias\n","        loss.backward()\n","        # Updates the weight and bias according to calculated gradient value\n","        optimizer.step()\n","      \n","        # Saves the number of epochs we trained for  \n","        checkpoint['epochs'] = n_epochs\n","        # Saves the models parameters\n","        checkpoint['model_state_dict'] = model.state_dict()\n","        # Saves the optimizers paramters\n","        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n","        # Saves the loss for the last batch so ultimately this will be the loss for the last batch of the last epoch\n","        checkpoint['loss'] = loss\n","        # Accumulates the loss\n","        cost+=loss.item()\n","        \n","     \n","    # Counter for the correct number of predictions        \n","    correct = 0\n","        \n","    # For each batch in the validation dataset\n","    for x_test, y_test in validation_loader:\n","        # Make a prediction\n","        z = model_r(x_test)\n","        # Get the class that has the maximum value\n","        _, yhat = torch.max(z.data, 1)\n","        # Counts the number of correct predictions made\n","        correct += (yhat == y_test).sum().item()\n"," \n","    accuracy = correct / N_test\n","    print(accuracy)\n","    # Appends the cost of the epoch to a list\n","    checkpoint['cost'].append(cost) \n","    # Appends the accuracy of the epoch to a list\n","    checkpoint['accuracy'].append(accuracy)\n","    # Saves the data in checkpoint to the file location\n","    torch.save(checkpoint, file_rotated) "]},{"cell_type":"markdown","id":"a31b1531-8a0e-4f08-a3ff-ad3d2a15d852","metadata":{},"source":["<h3 id=\"Result\">Analyze Results</h3> \n"]},{"cell_type":"markdown","id":"24be9890-2a4b-4969-8944-8a9bdaab332c","metadata":{},"source":["Loads the data which is saved in `rotated_data.pt`\n"]},{"cell_type":"code","execution_count":null,"id":"5287134a-5eb4-4984-9241-19cd92ca6d1a","metadata":{},"outputs":[],"source":["checkpoint_rotated= torch.load(os.path.join(os.getcwd(),'rotated_data.pt'))"]},{"cell_type":"markdown","id":"046a7b05-437f-4646-aeeb-ea7a337fe1d0","metadata":{},"source":["Plot Accuracy and Cost vs Epoch Graph\n"]},{"cell_type":"code","execution_count":null,"id":"1e1480af-9ac9-4b8d-91ac-3d52b363ae1c","metadata":{},"outputs":[],"source":["# Using the helper function defined at the top and the cost and accuracy lists that we saved\n","plot_cost_accuracy(checkpoint_rotated)"]},{"cell_type":"markdown","id":"f6852d6c-3918-4b9d-90e7-d456ad5157ec","metadata":{},"source":["Five misclassified samples\n"]},{"cell_type":"code","execution_count":null,"id":"49eec1cb-f623-4685-8117-2af80c60f0a4","metadata":{},"outputs":[],"source":["# Using the model parameters we saved we load them into a model to recreate the trained model\n","model_r.load_state_dict(checkpoint_rotated['model_state_dict'])\n","# Setting the model to evaluation mode\n","model.eval()\n","# Using the helper function plot the first five misclassified samples\n","plot_mis_classified(model_r,validation_dataset)"]},{"cell_type":"markdown","id":"93fc55d0-a59b-46c9-a752-a804b3a24b9f","metadata":{},"source":["## Summary \n"]},{"cell_type":"markdown","id":"0d23714a-b8a9-4e52-bd65-6ed0a9458182","metadata":{},"source":["As you can see in the two Cost and Accuracy vs Epoch Graph the model trained on rotated data performs much better on the rotated validation data, 79% vs 95%+. By creating and training on rotated data allows the model to learn from unique data and have increased exposure to real life situations because data is not always perfect or rotated correctly.\n"]},{"cell_type":"markdown","id":"bb78ffdb-6907-4c50-9450-3d5101d0e7ac","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"92745625-da3a-43a7-89da-dd23f820b522","metadata":{},"source":["<a href=\"http://cocl.us/pytorch_link_bottom\">\n","    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/notebook_bottom%20.png\" width=\"750\" alt=\"PyTorch Bottom\">\n","</a>\n"]},{"cell_type":"markdown","id":"fdda2ada-0ada-4010-86f2-2aafbd6d18fd","metadata":{},"source":["<h2>About the Authors:</h2> \n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"545eeb5e-bd05-45b4-81fa-7263e47203cb","metadata":{},"source":["Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01\">Michelle Carey</a>, <a href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a>\n"]},{"cell_type":"markdown","id":"6ddbe1a0-63ed-466d-b432-ec80f9d7f03f","metadata":{},"source":["Thanks to Magnus <a href=\"http://www.hvass-labs.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01\">Erik Hvass Pedersen</a> whose tutorials helped me understand Convolutional Neural Networks\n"]},{"cell_type":"markdown","id":"06247df8-06a2-43bc-a66a-b687f43eb884","metadata":{},"source":["<hr>\n"]},{"cell_type":"markdown","id":"9cd2342c-09db-49e6-b13d-41e755d79033","metadata":{},"source":["Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01\">MIT License</a>.\n"]}],"metadata":{"kernelspec":{"display_name":"tensorflow1","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"}},"nbformat":4,"nbformat_minor":4}
